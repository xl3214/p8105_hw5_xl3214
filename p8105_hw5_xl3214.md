p8105_hw5_xl3214
================
Xuan Lu
2023-11-14

# Question 1

## Introduction

For this analysis segment, I delved into the grim reality of unsolved
homicides across various cities in the United States. Drawing
inspiration from a [2018 Washington Post
investigation](https://www.washingtonpost.com/graphics/2018/investigations/where-murders-go-unsolved/)
which highlighted the concerning rates of murders going unsolved,
particularly in certain urban areas, this report seeks to illuminate
patterns in unsolved homicide cases. Utilizing a comprehensive dataset
detailing homicides, this analysis not only identifies cities with high
proportions of unsolved cases but also examines the demographic and
geographical nuances underlying these statistics. The rationale behind
this study stems from the urgent need to address and understand the
factors contributing to the lack of resolution in these cases, as this
has profound implications for justice and public safety.

## Data Analysis

``` r
library(tidyverse)
library(dplyr)
library(broom)
library(ggplot2)
```

``` r
homicide_raw <- read.csv(file = "data_files/homicide-data.csv")
```

The raw data has 12 columns and 52179 rows. It contains variables *uid,
reported_date, victim_last, victim_first, victim_race, victim_age,
victim_sex, city, state, lat, lon, disposition*.

``` r
homicide <- homicide_raw |>
  janitor::clean_names() |>
  mutate(city_state = paste(city, state, sep = ", "),
         victim_race = as.factor(victim_race), 
         victim_sex = as.factor(victim_sex), 
         victim_age = as.factor(victim_age), 
         disposition = as.factor(disposition))
summary(homicide)
```

    ##      uid            reported_date       victim_last        victim_first      
    ##  Length:52179       Min.   : 20070101   Length:52179       Length:52179      
    ##  Class :character   1st Qu.: 20100318   Class :character   Class :character  
    ##  Mode  :character   Median : 20121216   Mode  :character   Mode  :character  
    ##                     Mean   : 20130899                                        
    ##                     3rd Qu.: 20150911                                        
    ##                     Max.   :201511105                                        
    ##                                                                              
    ##    victim_race      victim_age      victim_sex        city          
    ##  Asian   :  685   Unknown: 2999   Female : 7209   Length:52179      
    ##  Black   :33361   22     : 2116   Male   :40739   Class :character  
    ##  Hispanic: 6901   21     : 2097   Unknown: 4231   Mode  :character  
    ##  Other   :  700   23     : 2061                                     
    ##  Unknown : 4199   24     : 1995                                     
    ##  White   : 6333   19     : 1976                                     
    ##                   (Other):38935                                     
    ##     state                lat             lon         
    ##  Length:52179       Min.   :25.73   Min.   :-122.51  
    ##  Class :character   1st Qu.:33.77   1st Qu.: -96.00  
    ##  Mode  :character   Median :38.52   Median : -87.71  
    ##                     Mean   :37.03   Mean   : -91.47  
    ##                     3rd Qu.:40.03   3rd Qu.: -81.76  
    ##                     Max.   :45.05   Max.   : -71.01  
    ##                     NA's   :60      NA's   :60       
    ##                 disposition     city_state       
    ##  Closed by arrest     :25674   Length:52179      
    ##  Closed without arrest: 2922   Class :character  
    ##  Open/No arrest       :23583   Mode  :character  
    ##                                                  
    ##                                                  
    ##                                                  
    ## 

``` r
homicide_sum <- homicide |> 
  group_by(city_state, disposition) |>
  summarise(total_homicides = n())
homicide_sum |> 
  pivot_wider(names_from = disposition, values_from = total_homicides) |>
  knitr::kable()
```

| city_state         | Closed by arrest | Closed without arrest | Open/No arrest |
|:-------------------|-----------------:|----------------------:|---------------:|
| Albuquerque, NM    |              232 |                    52 |             94 |
| Atlanta, GA        |              600 |                    58 |            315 |
| Baltimore, MD      |             1002 |                   152 |           1673 |
| Baton Rouge, LA    |              228 |                    16 |            180 |
| Birmingham, AL     |              453 |                    64 |            283 |
| Boston, MA         |              304 |                    NA |            310 |
| Buffalo, NY        |              202 |                     8 |            311 |
| Charlotte, NC      |              481 |                    44 |            162 |
| Chicago, IL        |             1462 |                   387 |           3686 |
| Cincinnati, OH     |              385 |                    49 |            260 |
| Columbus, OH       |              509 |                    80 |            495 |
| Dallas, TX         |              813 |                    78 |            676 |
| Denver, CO         |              143 |                    46 |            123 |
| Detroit, MI        |             1037 |                    16 |           1466 |
| Durham, NC         |              175 |                    11 |             90 |
| Fort Worth, TX     |              294 |                    35 |            220 |
| Fresno, CA         |              318 |                    23 |            146 |
| Houston, TX        |             1449 |                   346 |           1147 |
| Indianapolis, IN   |              728 |                   102 |            492 |
| Jacksonville, FL   |              571 |                   141 |            456 |
| Kansas City, MO    |              704 |                    36 |            450 |
| Las Vegas, NV      |              809 |                   175 |            397 |
| Long Beach, CA     |              222 |                    27 |            129 |
| Los Angeles, CA    |             1151 |                    NA |           1106 |
| Louisville, KY     |              315 |                    NA |            261 |
| Memphis, TN        |             1031 |                    50 |            433 |
| Miami, FL          |              294 |                    63 |            387 |
| Milwaukee, wI      |              712 |                    37 |            366 |
| Minneapolis, MN    |              179 |                    31 |            156 |
| Nashville, TN      |              489 |                    57 |            221 |
| New Orleans, LA    |              504 |                    98 |            832 |
| New York, NY       |              384 |                    17 |            226 |
| Oakland, CA        |              439 |                    NA |            508 |
| Oklahoma City, OK  |              346 |                    11 |            315 |
| Omaha, NE          |              240 |                    10 |            159 |
| Philadelphia, PA   |             1677 |                    92 |           1268 |
| Phoenix, AZ        |              410 |                    96 |            408 |
| Pittsburgh, PA     |              294 |                    NA |            337 |
| Richmond, VA       |              316 |                    20 |             93 |
| Sacramento, CA     |              237 |                    23 |            116 |
| San Antonio, TX    |              476 |                    87 |            270 |
| San Bernardino, CA |              105 |                    19 |            151 |
| San Diego, CA      |              286 |                    64 |            111 |
| San Francisco, CA  |              327 |                     1 |            335 |
| Savannah, GA       |              131 |                    12 |            103 |
| St.Â Louis, MO      |              772 |                    40 |            865 |
| Stockton, CA       |              178 |                    11 |            255 |
| Tampa, FL          |              113 |                     8 |             87 |
| Tulsa, AL          |                1 |                    NA |             NA |
| Tulsa, OK          |              390 |                    55 |            138 |
| Washington, DC     |              756 |                    74 |            515 |

``` r
proportion_test_by_city <- function(data, city_column, disposition_column) {
  data |>
    group_by({{ city_column }}) |>
    summarize(
      unsolved_count = sum({{ disposition_column }} %in% c("Closed without arrest", "Open/No arrest"), na.rm = TRUE),
      total_cases = n(),
      prop_test_result = list(prop.test(unsolved_count, total_cases, correct = FALSE))
    ) |>
    mutate(tidy_result = map(prop_test_result, broom::tidy)) |>
    select(-prop_test_result) |>
    unnest(tidy_result) |>
    select(city = {{ city_column }}, estimate, conf.low, conf.high)
}

homicide_summary <- proportion_test_by_city(homicide, city_state, disposition)

# Sort cities by the estimated proportion of unsolved homicides
homicide_summary_sorted <- homicide_summary |>
  arrange(desc(estimate))

# Create the plot
proportion_plot <- ggplot(homicide_summary_sorted, aes(x = reorder(city, estimate), y = estimate)) +
  geom_point() +  # Add points for estimates
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +  # Add error bars
  coord_flip() +  # Flip coordinates to make it easier to read city names
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme_minimal()
```

## Findings and Interpretation

``` r
proportion_plot
```

![](p8105_hw5_xl3214_files/figure-gfm/Q1-proportion%20plot-1.png)<!-- -->

The analysis revealed significant variations in the proportion of
unsolved homicides across different cities. Notably, cities like
Baltimore, Detroit, and Chicago emerged with a higher percentage of
unsolved cases, aligning with national trends reported in major
investigations. This pattern raises critical questions about law
enforcement efficacy, resource allocation, and possible systemic issues
within these urban centers.

The data also brought to light the demographic aspects of the victims,
showing disparities in the resolution of cases based on factors like
race and age. Such disparities point towards potential underlying biases
or systemic challenges in the criminal justice system that need
addressing.

# Question 2

## Introduction

For this part of the project, I am working with data from a study that
tracked two groups over time: one that received a treatment and one that
did not. You can download the data
[HERE](https://p8105.com/data/hw5_data.zip). Each person in the study
has their own file, named with a unique ID and which group they were in.
I combined all these files into one clean dataset so we can easily see
how each personâs results changed through the course of the study. I
also made a âspaghetti plotâ to show everyoneâs data over time in a way
that lets us compare the two groups.

## Data Analysis

``` r
library(tidyverse)

# List file names
file_names <- list.files(path = "data_files/question2", full.names = TRUE)

# Extract arm and subject ID information
file_info <- tibble(
  file_path = file_names,
  arm = if_else(str_detect(file_path, "con_"), "Control", "Experimental"),
  subject_id = str_extract(basename(file_path), "exp_\\d+|con_\\d+")
)

# Initialize an empty list to store dataframes
data_list <- list()

# Loop over each file
for(i in seq_along(file_info$file_path)) {
  # Read each file
  temp_data <- read_csv(file_info$file_path[i]) |> 
    mutate(subject_id = file_info$subject_id[i], arm = file_info$arm[i])
  
  # Append to the list
  data_list[[i]] <- temp_data
}

# Combine all dataframes into one
combined_data <- bind_rows(data_list)
```

The combined raw data has 10 columns and 20 rows. It contains variables
*week_1, week_2, week_3, week_4, week_5, week_6, week_7, week_8,
subject_id, arm*.

``` r
# Tidy the data
tidy_data <- combined_data |>
  pivot_longer(
    cols = -c(subject_id, arm),
    names_to = "week",
    values_to = "observation"
  )

summary(tidy_data)
```

    ##   subject_id            arm                week            observation    
    ##  Length:160         Length:160         Length:160         Min.   :-2.170  
    ##  Class :character   Class :character   Class :character   1st Qu.: 0.760  
    ##  Mode  :character   Mode  :character   Mode  :character   Median : 2.080  
    ##                                                           Mean   : 2.330  
    ##                                                           3rd Qu.: 3.603  
    ##                                                           Max.   : 7.660

``` r
# Spaghetti plot
spaghetti_plot <- ggplot(tidy_data, aes(x = week, y = observation, group = subject_id, color = arm)) +
  geom_line() +
  labs(title = "Observations Over Time by Subject", x = "Week", y = "Observation") +
  theme_minimal()
```

## Findings and Interpretation

``` r
spaghetti_plot
```

![](p8105_hw5_xl3214_files/figure-gfm/Q2-spaghetti%20plot-1.png)<!-- -->

The experimental group displayed a range of responses, illustrating the
natural variability of human reactions to the treatment. Over the course
of the study, these responses showed notable fluctuations, suggesting
that the treatmentâs impact varied over time. Conversely, the control
group served as a consistent baseline, with their results remaining
relatively stable, reflecting the expected outcome in the absence of the
treatment.

# Question 3

## Introduction

In Question 3, we are looking at statistical power â the chance that our
test will spot a real difference if there is one. This depends on things
like how many data points we have, the size of the effect we are looking
for, and how much our data varies. I have set up a simulation to explore
this.

Using 30 data points each time and a variability of 5, Iâll simulate
5000 experiments with normal data where the average is usually zero.
Then, I will check what happens to our estimates and chances of finding
a significant result when the real average changes from 1 to 6. This
will help us understand how well our test can detect actual differences,
which is crucial for setting up good experiments.

## Data Analysis

``` r
n <- 30      # Sample size
sigma <- 5   # Standard deviation (Ï)
mu <- 0      # Population mean (Î¼)
alpha <- 0.05 # Significance level
```

``` r
set.seed(123)  # For reproducibility

# Function to generate datasets and perform t-tests
simulate_t_test <- function(mu) {
  t_tests <- replicate(5000, {
    sample <- rnorm(n, mean = mu, sd = sigma)
    t_test_result <- t.test(sample, mu = 0)
    broom::tidy(t_test_result)
  }, simplify = FALSE)
  
  t_tests_df <- bind_rows(t_tests, .id = "dataset_id") |>
    mutate(true_mu = mu)
  
  return(t_tests_df)
}

# List of mu values to test
mu_values <- 0:6

# Perform simulations
simulations <- map(mu_values, ~ simulate_t_test(.x)) |>
  bind_rows() |>
  group_by(true_mu) |>
  summarise(
    power = mean(p.value < alpha),
    avg_estimate = mean(estimate),
    avg_estimate_rejected = mean(estimate[p.value < alpha])
  )
```

``` r
# Plot showing the power of the test
power_plot <- ggplot(simulations, aes(x = true_mu, y = power)) +
  geom_point() +
  geom_line() +
  labs(title = "Power vs Effect Size",
       x = "True Value of Î¼",
       y = "Power of the Test") +
  theme_minimal()

# Plot showing the average estimate of Î¼Ì
estimate_plot <- ggplot(simulations, aes(x = true_mu)) +
  geom_point(aes(y = avg_estimate, color = "Overall Average")) +
  geom_line(aes(y = avg_estimate, color = "Overall Average")) +
  geom_point(aes(y = avg_estimate_rejected, color = "Average When Null Rejected")) +
  geom_line(aes(y = avg_estimate_rejected, color = "Average When Null Rejected")) +
  scale_color_manual(
    name = "Estimate Type",
    values = c("Overall Average" = "blue", "Average When Null Rejected" = "red")
  ) +
  labs(title = "Average Estimate of Î¼Ì",
       x = "True Value of Î¼",
       y = "Average Estimate of Î¼Ì") +
  theme_minimal()
```

## Findings and Interpretation

``` r
power_plot
```

![](p8105_hw5_xl3214_files/figure-gfm/Q3-power%20plot-1.png)<!-- -->

Power vs True Mean Plot: This plot depicts how power increases with the
true mean, Î¼. The effect size, which is the difference between the true
mean and the null hypothesis mean, is directly proportional to the
power. As the true mean increases from 0 to 6, we should observe an
increase in power, indicating a greater likelihood of correctly
rejecting the false null hypothesis.

``` r
estimate_plot
```

![](p8105_hw5_xl3214_files/figure-gfm/Q3-estimate%20plot-1.png)<!-- -->

Average Estimate of Î¼Ì Plots: The red line shows the average estimate of
Î¼Ì across all simulations at different true mean values. The blue line
shows the average estimate of Î¼Ì only for those simulations where the
null hypothesis was rejected. The average of Î¼Ì where the null is
rejected is closer to the true Î¼, but due to the selection of only
significant results, it is biased and not exactly equal to the true
mean. This phenomenon is known as the âwinnerâs curseâ in statistical
literature.
